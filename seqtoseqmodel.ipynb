{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seqtoseqmodel.ipynb",
      "provenance": [],
      "mount_file_id": "15GGXfWVTHOseTiQqHCswwlCLf7M1n8n_",
      "authorship_tag": "ABX9TyMei5lP0W/omwfManIeG1I8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrFranciscoPaz/restmex/blob/main/seqtoseqmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2h1orv0Mep6",
        "outputId": "8c0051bd-f0fc-49e8-c0b8-2aceaa5b7090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import nltk\n",
        "\n",
        "import gensim\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "import re\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder = './drive/MyDrive/data/'\n",
        "namefiles = os.listdir(folder)"
      ],
      "metadata": {
        "id": "t0RwwemMNJwI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [\"covid\"]\n",
        "spanishSW = stopwords.words('spanish')\n"
      ],
      "metadata": {
        "id": "8yW-GvamNrZJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PrepararOraciones(str_Oracion):\n",
        "  str_Oracion = str_Oracion.lower().strip()\n",
        "\n",
        "  # Se crea un espacio entre las palabras y signos de puntuación\n",
        "  str_Oracion = re.sub(r\"([?.!,¿])\", r\" \\1 \", str_Oracion)\n",
        "  str_Oracion = re.sub(r'[\" \"]+', \" \", str_Oracion)\n",
        "\n",
        "  # Se reemplaza todo por un espacio excepto letras y signos especificados\n",
        "  str_Oracion = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", str_Oracion)\n",
        "\n",
        "  str_Oracion = str_Oracion.strip()\n",
        "\n",
        "  # Se agrega el token de inicio y fin\n",
        "  str_Oracion = '<start> ' + str_Oracion + ' <end>'\n",
        "\n",
        "  with open(folder + file, encoding = \"UTF-8\") as f:\n",
        "        lines = f.readlines()\n",
        "  news = ' '.join(lines)\n",
        "  news = news.split(\"#END\")\n",
        "  news = [x for x in news for y in keywords if y in x]\n",
        "  news = ' '.join(lines)\n",
        "  news = news.replace(\"#\", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
        "  news = news.replace(\";\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "  news = news.replace(\":\", \"\").replace(\"@\", \"\").replace(\"|\", \"\")\n",
        "  news = news.split(\" \")\n",
        "  #words = [re.sub(r'\"!\\\"$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~', \"\", x) for x in words]\n",
        "  \n",
        "  proc = [x.lower() for x in news if x not in sw]\n",
        "  news = ' '.join(proc)\n",
        "  return str_Oracion"
      ],
      "metadata": {
        "id": "T2NoY7olNKHE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenizar(tupl_Frases):\n",
        "  \n",
        "  Tokenizador_Frases = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  Tokenizador_Frases.fit_on_texts(tupl_Frases)\n",
        "\n",
        "  tensor = Tokenizador_Frases.texts_to_sequences(tupl_Frases)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, Tokenizador_Frases"
      ],
      "metadata": {
        "id": "A3cCCxaONrd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CargarDataset(str_RutaArchivo, nbr_CantEjemplos=None):\n",
        "  \n",
        "  tupl_Target, tupl_Input = PrepararDataset(str_RutaArchivo, nbr_CantEjemplos)\n",
        "\n",
        "  # Idioma Input\n",
        "  tensor_Input, Tokenizador_Frases_Input = Tokenizar(tupl_Input)\n",
        "  \n",
        "  # Idioma Target\n",
        "  tensor_Target, Tokenizador_Frases_Target = Tokenizar(tupl_Target)\n",
        "\n",
        "  return tensor_Input, tensor_Target, Tokenizador_Frases_Input, Tokenizador_Frases_Target"
      ],
      "metadata": {
        "id": "POgrzRFbNrgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nbr_CantEjemplos = 100\n",
        "# nbr_CantEjemplos = 118964 \n",
        "\n",
        "tensor_Input, tensor_Target, Tokenizador_Frases_Input, Tokenizador_Frases_Target = CargarDataset(str_RutaArchivo, nbr_CantEjemplos)\n",
        "\n",
        "# Obtenemos el tamanio máximo de los tensores\n",
        "nbr_TamanioMax_FraseTarget, nbr_TamanioMax_FraseInput = tensor_Target.shape[1], tensor_Input.shape[1]"
      ],
      "metadata": {
        "id": "LjGgdcrONrid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7j746ZV9PQUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RykR_ermPQWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Quitar acentos\n",
        "# 2. Limpiar oraciones\n",
        "# 3. Regresar parejas de textos en el formtato: [Target, Input]\n",
        "def PrepararDataset(str_RutaArchivo, nbr_CantEjemplos):\n",
        "\n",
        "  str_Lineas = io.open(str_RutaArchivo, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  list_ParejaDeFrases = [[PrepararOraciones(str_Oracion) for str_Oracion in str_Linea.split('\\t')]  for str_Linea in str_Lineas[:nbr_CantEjemplos]]\n",
        "  \n",
        "  return zip(*list_ParejaDeFrases)"
      ],
      "metadata": {
        "id": "PF1KwdUHPQZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pojjdepwPQbQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}